# Adaptive Hamiltonian Monte Carlo (<a name='ahmc'>AHMC</a>)

<br>
```{r, echo=FALSE}
pander::pander(
  data.frame(
    Aspect = c('Acceptance Rate', 'Applications', 'Difficulty', 'Final Algorithm?', 'Proposal'),
    Description = c('The optimal acceptance rate is 65% when `L` > 1, or 57.4% when `L` = 1. The observed acceptance rate may be suitable in the interval [60%,70%] when `L` > 1, or [40%,80%] when `L` = 1.', 'This is a widely applicable, general-purpose algorithm that is best suited to models with a small number of parameters. The number of model evaluations per iteration increases with the number of parameters.','This algorithm is relatively difficult for a beginner. It has four algorithm specifications. Even though it is adaptive, it is difficult to tune. Since it is an adaptive algorithm, the user must regard diminishing adaptation.','User Discretion. The HMC algorithm is recommended as the final algorithm, though AHMC may be used if diminishing adaptation occurs and adaptation ceases effectively.', 'Multivariate. Proposals are multivariate only in the sense that proposals for multiple parameters are generated at once. However, proposals are not generated with a multivariate distribution and a proposal covariance matrix. Each iteration involves numerous proposals, due to partial derivatives and `L`.')),
  justify='left', style='multiline', split.cells=c(17,83), split.tables=Inf)
```

<br>


This is an adaptive form of Hamiltonian Monte Carlo (HMC) called Adaptive Hamiltonian Monte Carlo (AHMC). In AHMC, the ε parameter is adapted, though `L` and `M` are not. When adapting, and considering K parameters, AHMC multiplies εk by 0.8 when a proposal for parameter sk has not been accepted in the last 10 iterations, or multiplies it by 1.2 when a proposal has been accepted at least 8 of the last 10 iterations, as suggested by Neal (2011).

AHMC has four algorithm specifications: `epsilon` or ε is the step-size, `L` is the number of leapfrog steps, `M` is an optional mass matrix, and `Periodicity` is the frequency in iterations for adaptation of ε.

Although AHMC generates multivariate proposals, parameter correlation is not taken into account unless a mass matrix is used. Some sources refer to a mass matrix in HMC as a covariance matrix, and some as a precision matrix. Here, the mass matrix is a covariance matrix. It is difficult to tune for several reasons. The optimal mass matrix is different with different configurations of the parameters, so what was optimal at one point is sub-optimal at another. It is not recommended to substitute the historical covariance matrix of samples while pursuing equilibrium.

As with HMC, the AHMC algorithm is slower per iteration than many other algorithms, but often produces chains with good mixing when well-tuned. An alternative to AHMC that should perform better is HMCDA. AHMC is more consistent with respect to time per iteration, because `L` remains constant, than HMCDA and NUTS, which may have some iterations that are much slower than others. If AHMC is used for adaptation, then the final, non-adaptive algorithm should be HMC.
References

## References

- Neal R (2011). "MCMC for Using Hamiltonian Dynamics." In S Brooks, A Gelman, G Jones, M Xiao-Li (eds.), Handbook of Markov Chain Monte Carlo, p. 113-162. Chapman & Hall, Boca Raton, FL.

## See Also

- [HMC](#hmc)
- [HMCDA](#hmcda)
- [MALA](#mala)
- [NUTS](#nuts)
- [THMC](#thmc)

