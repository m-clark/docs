# Model every group

We could also run a regression for every group.  This is problematic since we would typically have so little data per group, and too many groups to make sense of.  We'll go ahead and do it anyway.  What if we take the average of all the estimates?

```{r lmList}
library(nlme)
lottalm = lmList(y ~ time | id, d)
colMeans(coef(lottalm))
coef(lm(y ~ time, data=d, x=T))
```

Surprise! We get the same result as if we had simply run the standard linear model (I abbreviate this as SLiM sometimes).  However, we would not get identical coefficients if the data are unbalanced, where individuals may have a different numbers of observations. 

```{r comparelmList, eval=F, echo=FALSE}
idxUB = sample(1:n*tp, 500)
d_UB = d[-idxUB,]
lottalm = lmList(y ~ time | id, d_UB)
colMeans(coef(lottalm), na.rm=T)
# d3heatmap::d3heatmap(coef(lottalm), Rowv=F, Colv=F)
coef(lm(y ~ time, d_UB))
```

```{r meanslope, eval=FALSE, echo=FALSE}
# mean of all possible slopes
n_ = 10000
z = rnorm(n_)
q = .5*z + rnorm(n_, sd=.1)
slopes = rep(NA, n_-1)
for(i in 1:(n_-1)){
  for(j in 1+i){
    slopes[i] = coef(lm(q[c(i,j)] ~ z[c(i,j)] -1))
  }
}
# allpairs = combn(1:n_, 2)
# mean(apply(allpairs, 2, function(x) coef(lm(q[x]~z[x]-1))[1]))
mean(slopes)
coef(lm(q ~ z))[2]
```

In general, this is not the way we want to do things, and one of the biggest drawbacks is that we can never examine cluster level covariates, which may be of key interest, nor is there a straightforward way to do inference for this scenario.  As we will see later, techniques are available that serve as a compromise between these first two alternatives of ignoring the clustering and overfitting to each cluster.

## Pros

- None really. 

## Cons

- Inefficient
- Overly complex
- Overfit at each cluster
- High variance in estimates at each cluster
- Ignores cluster level effects

Gist: While it might be a fun exercise, there is little to be gained by this approach.
